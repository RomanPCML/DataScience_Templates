{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python361064bitmlazcondaea96736406454e8bbc76c8ba61cb2ddd",
   "display_name": "Python 3.6.10 64-bit ('ml_az': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Multiple Linear Regression </h2>\n",
    "\n",
    "<h4> Methods of building a model <h5>\n",
    "\n",
    "<h4>All-In</h4>\n",
    "<p> The \"All-In\" Method simple takes all given variables and builds a model from those. This should only be the case if you have prior knowledge, the variable are suitable and necessary for the regression or as a starting point for the Backward Eleimination</p>\n",
    "\n",
    "<h4> Backward Elimination (shown in this Notebook)</h4>\n",
    "<ol>\n",
    "<li> Select a sigificance level to keep variable (eg. 0.05) </li>\n",
    "<li> Fit the full model with all predictors </li>\n",
    "<li> Consider the predictor with the highest P-Value. If p>sig  --> remove variable and go to 4. Otherweise finish process </li>\n",
    "<li> Remove the predictor </li>\n",
    "<li> Fit the model without the predictor and go back to step 3</li>\n",
    "<ol>\n",
    "\n",
    "<h4> Forward Selection  </h4>\n",
    "<ol>\n",
    "<li>Select a significance level </li>\n",
    "<li>Fit simple regression model to ALL independent variables and select variable with lowest p-value</li>\n",
    "<li>Keep this variable and fit all possible models with this additional variable </li>\n",
    "<li>Consider the predictor with the lowers p-value. If p < sig go to step 3. Otherwise finish</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "<h4> Bidirectional Elimination </h4>\n",
    "<ol>\n",
    "<li> Select significance level to enter and to stay in the model</li>\n",
    "<li> Perform next step of Forward selection (p < sig.enter) </li>\n",
    "<li> Perform ALL steps of Backward Elimination (p < p.stay) </li>\n",
    "<li> No new variables can enter and no old variables can exit. Finish </li>\n",
    "\n",
    "<h4> All possible models </h4>\n",
    "VERY ressource consuming\n",
    "<ol>\n",
    "<li> Select criterion od goodness of fit (R2 etc.)</li>\n",
    "<li> Calculate all possible models in the data (2^n -1 models ) </li>\n",
    "<li> Select model with best fit </li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "os.chdir(\"C:/Users/work/Documents/Training/Udemy/MachineLearning A-Z/original/Machine Learning A-Z New/Part 2 - Regression/Section 5 - Multiple Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Preprocessing</h2>\n",
    "<ol>\n",
    "<li>Import data</li>\n",
    "<li>Seperate dependent variabel for independet variables</li>\n",
    "<li>Create dummy variables for catigorical data</li>\n",
    "<li>Split into trainign and test data</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>R&amp;D Spend</th>\n      <th>Administration</th>\n      <th>Marketing Spend</th>\n      <th>State</th>\n      <th>Profit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>165349.20</td>\n      <td>136897.80</td>\n      <td>471784.10</td>\n      <td>New York</td>\n      <td>192261.83</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>162597.70</td>\n      <td>151377.59</td>\n      <td>443898.53</td>\n      <td>California</td>\n      <td>191792.06</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>153441.51</td>\n      <td>101145.55</td>\n      <td>407934.54</td>\n      <td>Florida</td>\n      <td>191050.39</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>144372.41</td>\n      <td>118671.85</td>\n      <td>383199.62</td>\n      <td>New York</td>\n      <td>182901.99</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>142107.34</td>\n      <td>91391.77</td>\n      <td>366168.42</td>\n      <td>Florida</td>\n      <td>166187.94</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   R&D Spend  Administration  Marketing Spend       State     Profit\n0  165349.20       136897.80        471784.10    New York  192261.83\n1  162597.70       151377.59        443898.53  California  191792.06\n2  153441.51       101145.55        407934.54     Florida  191050.39\n3  144372.41       118671.85        383199.62    New York  182901.99\n4  142107.34        91391.77        366168.42     Florida  166187.94"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('50_Startups.csv')\n",
    "# take all clumns exept last one (dependent variable)\n",
    "X = dataset.iloc[:, :-1].values\n",
    "# only take last column\n",
    "y = dataset.iloc[:, -1].values\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variable for catigorical \"State\" variable\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer([('encoder', OneHotEncoder(), [3])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X), dtype=np.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid Dummy Variable Trap (remove one dummy variable)#\n",
    "# no need to do it manuall, just for understanding\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Fitting Multiple Linear Regression to Training set </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Predict Test Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the test set\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Build optimal model using Backward Elimintation</h2>\n",
    "<p> Reminder: </p>\n",
    "<ol>\n",
    "<li> Select a sigificance level to keep variable (eg. 0.05) </li>\n",
    "<li> Fit the full model with all predictors </li>\n",
    "<li> Consider the predictor with the highest P-Value. If p>sig  --> remove variable and go to 4. Otherweise finish process </li>\n",
    "<li> Remove the predictor </li>\n",
    "<li> Fit the model without the predictor and go back to step 3</li>\n",
    "<ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column of 1s at the beginning of the matrix, this is required by the statmodels api (will act as intercept)\n",
    "X = np.append(arr=np.ones((50,1)).astype(int), values=X, axis=1)\n",
    "\n",
    "# create matrix of optimal independent variables (initally all columns of dataset)\n",
    "X_opt = X[:, [0,1,2,3,4,5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.947</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   849.8</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Thu, 12 Mar 2020</td> <th>  Prob (F-statistic):</th> <td>3.50e-32</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>15:07:39</td>     <th>  Log-Likelihood:    </th> <td> -527.44</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1059.</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    48</td>      <th>  BIC:               </th> <td>   1063.</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>const</th> <td> 4.903e+04</td> <td> 2537.897</td> <td>   19.320</td> <td> 0.000</td> <td> 4.39e+04</td> <td> 5.41e+04</td>\n</tr>\n<tr>\n  <th>x1</th>    <td>    0.8543</td> <td>    0.029</td> <td>   29.151</td> <td> 0.000</td> <td>    0.795</td> <td>    0.913</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>13.727</td> <th>  Durbin-Watson:     </th> <td>   1.116</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  18.536</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.911</td> <th>  Prob(JB):          </th> <td>9.44e-05</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 5.361</td> <th>  Cond. No.          </th> <td>1.65e+05</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.65e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems.",
      "text/plain": "<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.947\nModel:                            OLS   Adj. R-squared:                  0.945\nMethod:                 Least Squares   F-statistic:                     849.8\nDate:                Thu, 12 Mar 2020   Prob (F-statistic):           3.50e-32\nTime:                        15:07:39   Log-Likelihood:                -527.44\nNo. Observations:                  50   AIC:                             1059.\nDf Residuals:                      48   BIC:                             1063.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       4.903e+04   2537.897     19.320      0.000    4.39e+04    5.41e+04\nx1             0.8543      0.029     29.151      0.000       0.795       0.913\n==============================================================================\nOmnibus:                       13.727   Durbin-Watson:                   1.116\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               18.536\nSkew:                          -0.911   Prob(JB):                     9.44e-05\nKurtosis:                       5.361   Cond. No.                     1.65e+05\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.65e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\"\"\""
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 : set significance Level\n",
    "sig = 0.05\n",
    "\n",
    "#Step 2: fit first model to \"optimal\" data (oridinary least squares)\n",
    "regressor_OLS = sm.OLS(endog = y , exog = X_opt).fit()\n",
    "\n",
    "# Step 3: Find Variable with highest p value\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Delete Variable with highest p value (index 2)\n",
    "X_opt = X[:, [0,3]]\n",
    "\n",
    "# repeat steps 1-4 until all p-values are lower than 0.05\n",
    "# use cell above and this cell to reduce list in X_opt according to pvalue in statistics overview above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Implement Backward Elimination as Function</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def backwardElimination(x, sl):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "    x : Training Data with all Variables\n",
    "    sl : significance level\n",
    "\n",
    "    Output:\n",
    "    x : Training Data reduced to sigificant variables\n",
    "    \"\"\"\n",
    "    numVars = len(x[0])\n",
    "    for i in range(0, numVars):\n",
    "        regressor_OLS = sm.OLS(y, x).fit()\n",
    "        maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "        if maxVar > sl:\n",
    "            for j in range(0, numVars - i):\n",
    "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                    x = np.delete(x, j, 1)\n",
    "    regressor_OLS.summary()\n",
    "    return x\n",
    " \n",
    "SL = 0.05\n",
    "X_opt = X[:, [0, 1, 2, 3, 4, 5]]\n",
    "X_Modeled = backwardElimination(X_opt, SL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Add R-squared to function to improve result</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.950\nModel:                            OLS   Adj. R-squared:                  0.948\nMethod:                 Least Squares   F-statistic:                     450.8\nDate:                Thu, 12 Mar 2020   Prob (F-statistic):           2.16e-31\nTime:                        15:14:27   Log-Likelihood:                -525.54\nNo. Observations:                  50   AIC:                             1057.\nDf Residuals:                      47   BIC:                             1063.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       4.698e+04   2689.933     17.464      0.000    4.16e+04    5.24e+04\nx1             0.7966      0.041     19.266      0.000       0.713       0.880\nx2             0.0299      0.016      1.927      0.060      -0.001       0.061\n==============================================================================\nOmnibus:                       14.677   Durbin-Watson:                   1.257\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               21.161\nSkew:                          -0.939   Prob(JB):                     2.54e-05\nKurtosis:                       5.575   Cond. No.                     5.32e+05\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 5.32e+05. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "def backwardElimination(x, SL):\n",
    "    numVars = len(x[0])\n",
    "    temp = np.zeros((50,6)).astype(int)\n",
    "    for i in range(0, numVars):\n",
    "        regressor_OLS = sm.OLS(y, x).fit()\n",
    "        maxVar = max(regressor_OLS.pvalues).astype(float)\n",
    "        adjR_before = regressor_OLS.rsquared_adj.astype(float)\n",
    "        if maxVar > SL:\n",
    "            for j in range(0, numVars - i):\n",
    "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                    temp[:,j] = x[:, j]\n",
    "                    x = np.delete(x, j, 1)\n",
    "                    tmp_regressor = sm.OLS(y, x).fit()\n",
    "                    adjR_after = tmp_regressor.rsquared_adj.astype(float)\n",
    "                    if (adjR_before >= adjR_after):\n",
    "                        x_rollback = np.hstack((x, temp[:,[0,j]]))\n",
    "                        x_rollback = np.delete(x_rollback, j, 1)\n",
    "                        print (regressor_OLS.summary())\n",
    "                        return x_rollback\n",
    "                    else:\n",
    "                        continue\n",
    "    regressor_OLS.summary()\n",
    "    return x\n",
    " \n",
    "SL = 0.05\n",
    "X_opt = X[:, [0, 1, 2, 3, 4, 5]]\n",
    "X_Modeled = backwardElimination(X_opt, SL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}