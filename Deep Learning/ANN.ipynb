{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Artificial Neural Networks </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'2.1.0'"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Data Preprocessing </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[619 'France' 'Female' ... 1 1 101348.88]\n [608 'Spain' 'Female' ... 0 1 112542.58]\n [502 'France' 'Female' ... 1 0 113931.57]\n ...\n [709 'France' 'Female' ... 0 1 42085.58]\n [772 'Germany' 'Male' ... 1 0 92888.52]\n [792 'France' 'Female' ... 1 0 38190.78]]\n"
    }
   ],
   "source": [
    "# Data Preprocessing Template\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('../sample_data/Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Encode Catigorical data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding fpr binary  variable Gender\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X[:, 2] = le.fit_transform(X[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[619, 'France', 0, ..., 1, 1, 101348.88],\n       [608, 'Spain', 0, ..., 0, 1, 112542.58],\n       [502, 'France', 0, ..., 1, 0, 113931.57],\n       ...,\n       [709, 'France', 0, ..., 0, 1, 42085.58],\n       [772, 'Germany', 1, ..., 1, 0, 92888.52],\n       [792, 'France', 0, ..., 1, 0, 38190.78]], dtype=object)"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encoding for Variabel Country\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder=\"passthrough\")\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Split Dataset into Training dataset and Validation set </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Feature Scaling </h3>\n",
    "Feature Scaling is ABSOLUTELY necessary for Deeep Learning. this is why it will be applied to all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Build the Artificial Neural Network </h2>\n",
    "\n",
    "- Initialize ANN\n",
    "- Adding the input layer and first hidden layer\n",
    "- Adding the second hidden layer\n",
    "- Adding the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding th input layer and first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# units is number of neurons in first hidden layer\n",
    "# no rule of thumb for hyperparameter. experiment for better results\n",
    "# relu is the rectified linear units activation function\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add math can be used to add any kind of layer, including the output layer\n",
    "\n",
    "# activation function \"sigmoid\" is used because it returns the probability of the result being 1 or 0\n",
    "\n",
    "# for multiple classes in the output data the activation function has to softmax\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training the ANN </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAM ist chose because it can perform Stochastic gradient descent, which will update the weights to reduce the loss between predictions and real values\n",
    "\n",
    "# for binary values loss funtion ALWAYS has to be binary_crossentropy\n",
    "# for multiple classes it has to catigorical_crossentropy\n",
    "\n",
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 8000 samples\nEpoch 1/100\n8000/8000 [==============================] - 1s 145us/sample - loss: 0.5281 - accuracy: 0.7924\nEpoch 2/100\n8000/8000 [==============================] - 0s 50us/sample - loss: 0.4707 - accuracy: 0.7976\nEpoch 3/100\n8000/8000 [==============================] - 0s 40us/sample - loss: 0.4526 - accuracy: 0.8014\nEpoch 4/100\n8000/8000 [==============================] - 0s 41us/sample - loss: 0.4431 - accuracy: 0.8076\nEpoch 5/100\n8000/8000 [==============================] - 0s 40us/sample - loss: 0.4359 - accuracy: 0.8112\nEpoch 6/100\n8000/8000 [==============================] - 0s 39us/sample - loss: 0.4298 - accuracy: 0.8163\nEpoch 7/100\n8000/8000 [==============================] - 0s 40us/sample - loss: 0.4248 - accuracy: 0.8196\nEpoch 8/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.4203 - accuracy: 0.8217\nEpoch 9/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.4160 - accuracy: 0.8255\nEpoch 10/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.4126 - accuracy: 0.8280\nEpoch 11/100\n8000/8000 [==============================] - 0s 47us/sample - loss: 0.4101 - accuracy: 0.8295\nEpoch 12/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.4073 - accuracy: 0.8310\nEpoch 13/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.4047 - accuracy: 0.8321\nEpoch 14/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.4024 - accuracy: 0.8325\nEpoch 15/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.4000 - accuracy: 0.8339\nEpoch 16/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3970 - accuracy: 0.8336\nEpoch 17/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3950 - accuracy: 0.8351\nEpoch 18/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3914 - accuracy: 0.8366\nEpoch 19/100\n8000/8000 [==============================] - 0s 49us/sample - loss: 0.3882 - accuracy: 0.8360\nEpoch 20/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3845 - accuracy: 0.8389\nEpoch 21/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3816 - accuracy: 0.8407\nEpoch 22/100\n8000/8000 [==============================] - 0s 47us/sample - loss: 0.3783 - accuracy: 0.8420\nEpoch 23/100\n8000/8000 [==============================] - 0s 41us/sample - loss: 0.3756 - accuracy: 0.8451\nEpoch 24/100\n8000/8000 [==============================] - 0s 45us/sample - loss: 0.3722 - accuracy: 0.8465\nEpoch 25/100\n8000/8000 [==============================] - 0s 45us/sample - loss: 0.3690 - accuracy: 0.8487\nEpoch 26/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3666 - accuracy: 0.8509\nEpoch 27/100\n8000/8000 [==============================] - 0s 41us/sample - loss: 0.3641 - accuracy: 0.8509\nEpoch 28/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3611 - accuracy: 0.8530\nEpoch 29/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3588 - accuracy: 0.8541\nEpoch 30/100\n8000/8000 [==============================] - 0s 45us/sample - loss: 0.3567 - accuracy: 0.8549\nEpoch 31/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3542 - accuracy: 0.8565\nEpoch 32/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3522 - accuracy: 0.8555\nEpoch 33/100\n8000/8000 [==============================] - 0s 46us/sample - loss: 0.3501 - accuracy: 0.8566\nEpoch 34/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3481 - accuracy: 0.8580\nEpoch 35/100\n8000/8000 [==============================] - 0s 47us/sample - loss: 0.3463 - accuracy: 0.8591\nEpoch 36/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3443 - accuracy: 0.8583\nEpoch 37/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3431 - accuracy: 0.8574\nEpoch 38/100\n8000/8000 [==============================] - 0s 41us/sample - loss: 0.3417 - accuracy: 0.8596\nEpoch 39/100\n8000/8000 [==============================] - 0s 48us/sample - loss: 0.3407 - accuracy: 0.8594\nEpoch 40/100\n8000/8000 [==============================] - 0s 45us/sample - loss: 0.3400 - accuracy: 0.8586\nEpoch 41/100\n8000/8000 [==============================] - 0s 50us/sample - loss: 0.3391 - accuracy: 0.8593\nEpoch 42/100\n8000/8000 [==============================] - 0s 41us/sample - loss: 0.3386 - accuracy: 0.8614\nEpoch 43/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3377 - accuracy: 0.8621\nEpoch 44/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3376 - accuracy: 0.8624\nEpoch 45/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3370 - accuracy: 0.8614\nEpoch 46/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3366 - accuracy: 0.8619\nEpoch 47/100\n8000/8000 [==============================] - 0s 45us/sample - loss: 0.3364 - accuracy: 0.8626\nEpoch 48/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3365 - accuracy: 0.8627\nEpoch 49/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3362 - accuracy: 0.8636\nEpoch 50/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3356 - accuracy: 0.8629\nEpoch 51/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3359 - accuracy: 0.8629\nEpoch 52/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3352 - accuracy: 0.8646\nEpoch 53/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3354 - accuracy: 0.8636\nEpoch 54/100\n8000/8000 [==============================] - 0s 47us/sample - loss: 0.3353 - accuracy: 0.8640\nEpoch 55/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3352 - accuracy: 0.8633\nEpoch 56/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3349 - accuracy: 0.8651\nEpoch 57/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3347 - accuracy: 0.8640\nEpoch 58/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3350 - accuracy: 0.8658\nEpoch 59/100\n8000/8000 [==============================] - 0s 46us/sample - loss: 0.3347 - accuracy: 0.8644\nEpoch 60/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3347 - accuracy: 0.8640\nEpoch 61/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3342 - accuracy: 0.8646\nEpoch 62/100\n8000/8000 [==============================] - 0s 45us/sample - loss: 0.3341 - accuracy: 0.8644\nEpoch 63/100\n8000/8000 [==============================] - 0s 45us/sample - loss: 0.3344 - accuracy: 0.8655\nEpoch 64/100\n8000/8000 [==============================] - 0s 48us/sample - loss: 0.3342 - accuracy: 0.8651\nEpoch 65/100\n8000/8000 [==============================] - 0s 45us/sample - loss: 0.3343 - accuracy: 0.8668\nEpoch 66/100\n8000/8000 [==============================] - 0s 47us/sample - loss: 0.3339 - accuracy: 0.8649\nEpoch 67/100\n8000/8000 [==============================] - 0s 46us/sample - loss: 0.3341 - accuracy: 0.8654\nEpoch 68/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3339 - accuracy: 0.8648\nEpoch 69/100\n8000/8000 [==============================] - 0s 45us/sample - loss: 0.3340 - accuracy: 0.8645\nEpoch 70/100\n8000/8000 [==============================] - 0s 47us/sample - loss: 0.3340 - accuracy: 0.8643\nEpoch 71/100\n8000/8000 [==============================] - 0s 47us/sample - loss: 0.3340 - accuracy: 0.8645\nEpoch 72/100\n8000/8000 [==============================] - 0s 45us/sample - loss: 0.3343 - accuracy: 0.8655\nEpoch 73/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3341 - accuracy: 0.8655\nEpoch 74/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3334 - accuracy: 0.8652\nEpoch 75/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3336 - accuracy: 0.8662\nEpoch 76/100\n8000/8000 [==============================] - 0s 46us/sample - loss: 0.3336 - accuracy: 0.8651\nEpoch 77/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3333 - accuracy: 0.8643\nEpoch 78/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3333 - accuracy: 0.8654\nEpoch 79/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3334 - accuracy: 0.8659\nEpoch 80/100\n8000/8000 [==============================] - 0s 50us/sample - loss: 0.3329 - accuracy: 0.8673\nEpoch 81/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3333 - accuracy: 0.8660\nEpoch 82/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3330 - accuracy: 0.8664\nEpoch 83/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3333 - accuracy: 0.8650\nEpoch 84/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3330 - accuracy: 0.8659\nEpoch 85/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3332 - accuracy: 0.8649\nEpoch 86/100\n8000/8000 [==============================] - 0s 41us/sample - loss: 0.3330 - accuracy: 0.8651\nEpoch 87/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3330 - accuracy: 0.8648\nEpoch 88/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3326 - accuracy: 0.8656\nEpoch 89/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3331 - accuracy: 0.8651\nEpoch 90/100\n8000/8000 [==============================] - 0s 50us/sample - loss: 0.3324 - accuracy: 0.8660\nEpoch 91/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3328 - accuracy: 0.8654\nEpoch 92/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3329 - accuracy: 0.8645\nEpoch 93/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3329 - accuracy: 0.8648\nEpoch 94/100\n8000/8000 [==============================] - 0s 45us/sample - loss: 0.3329 - accuracy: 0.8664\nEpoch 95/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3328 - accuracy: 0.8662\nEpoch 96/100\n8000/8000 [==============================] - 0s 44us/sample - loss: 0.3330 - accuracy: 0.8652\nEpoch 97/100\n8000/8000 [==============================] - 0s 49us/sample - loss: 0.3328 - accuracy: 0.8637\nEpoch 98/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3327 - accuracy: 0.8649\nEpoch 99/100\n8000/8000 [==============================] - 0s 42us/sample - loss: 0.3326 - accuracy: 0.8670\nEpoch 100/100\n8000/8000 [==============================] - 0s 43us/sample - loss: 0.3323 - accuracy: 0.8646\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x238fec3fd30>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Making the prediction and evelute the model </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.03889995]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[False]]\n"
    }
   ],
   "source": [
    "# trans form resulst (probaility) into prediciton if costumer wil leave bank or not\n",
    "print(ann.predict(sc.transform([[1, 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Predicting the test set </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0 0]\n [0 1]\n [0 0]\n ...\n [0 0]\n [0 0]\n [0 0]]\n"
    }
   ],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "#transform probabilities to binary results\n",
    "\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Creating Confusion Matrix </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1508   87]\n [ 196  209]]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8585"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitmlazconda5dc0c2caba1d42ddbca8e014160a0469",
   "display_name": "Python 3.6.10 64-bit ('ml_az': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}